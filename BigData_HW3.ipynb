{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Big-data/blob/main/BigData_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<h1 dir=rtl>\n",
        "تمرین برنامه‌نویسی سوم\n",
        "</h1>\n",
        "<h3 dir=rtl>\n",
        "Clustering\n",
        "</h3>\n",
        "</center>"
      ],
      "metadata": {
        "id": "nCCFqMmmP5Db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "در این تمرین به دنبال خوشه‌بندی داده‌های لاگ‌های مربوط به حدود ۵ میلیون connection در شبکه اینترنتی هستیم. این خوشه‌بندی مرحله مقدماتی برای تشخیص رفتارهای غیرعادی و حملات احتمالی به شبکه است. به عبارت دیگر، پس از انجام این خوشه‌بندی به شکل مناسب، می‌توان برای اطلاعات جدید بررسی کرد که آیا فاصله داده‌های جدید نسبت به مرکز خوشه مربوطه از یک حد آستانه بیشتر است یا خیر. \n",
        "</p>"
      ],
      "metadata": {
        "id": "aEhMYKoIUnR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "برای انجام این پروژه از دیتای مسابقه kddcup که در سال ۱۹۹۹ انجام شده است استفاده می‌کنیم. \n",
        "<br>\n",
        "برای این منظور این دیتا را از اینجا دانلود کنید:\n",
        "<a href=\"https://www.kaggle.com/datasets/galaxyh/kdd-cup-1999-data\">https://www.kaggle.com/datasets/galaxyh/kdd-cup-1999-data</a>\n",
        "<br>\n",
        "فایلی که لازم است دانلود شود، فایل kddcup.data.gz\n",
        "است. دقت کنید که احتمالا برای دانلود فایل نیاز به فیلترشکن داشته باشید.\n",
        "<br>\n",
        "پس از دانلود فایل، آنرا در قسمت Files در سرور مجازی colab در سمت چپ همین فایل نوت‌بوک آپلود کنید.\n",
        "</p>"
      ],
      "metadata": {
        "id": "l8SfbfV0WJfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "در ادامه، فایل مورد نظر را unzip می‌کنیم:\n",
        "</p>"
      ],
      "metadata": {
        "id": "aaOlsr8KW_ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip kddcup.data.gz"
      ],
      "metadata": {
        "id": "pxd4d4CU1On2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=\"rtl\">\n",
        "چند سطر اول فایل csv ایجاد شده را می‌توانیم با دستور ذیل بررسی کنیم:\n",
        "</p>"
      ],
      "metadata": {
        "id": "gyaU3MrMXHgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head kddcup.data"
      ],
      "metadata": {
        "id": "MKYy5sBJQ6ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "در ادامه، پکیج pyspark را نصب کرده و تنظیمات اولیه آنرا انجام می‌دهیم:\n",
        "</p>"
      ],
      "metadata": {
        "id": "3DE9P08YXQHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "xU5lp3TuRGY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "HKI03L0HRTh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "حال، فایل دیتا را با دستور ذیل به یک DataFrame تبدیل می‌کنیم.\n",
        "<br>\n",
        "همچنین با توجه به اینکه در سطر اول این فایل اطلاعات هدر مشخص نشده است، این اطلاعات را به صورت دستی به DataFrame اضافه می‌کنیم:\n",
        "</p>"
      ],
      "metadata": {
        "id": "b0lg9-DqXbHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('kddcup.data', inferSchema=True, header=False)"
      ],
      "metadata": {
        "id": "iML6g1m5Rayd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"duration\", \"protocol_type\", \"service\", \"flag\",\n",
        "      \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
        "      \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
        "      \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
        "      \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "      \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
        "      \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
        "      \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
        "      \"dst_host_count\", \"dst_host_srv_count\",\n",
        "      \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
        "      \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
        "      \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "      \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
        "      \"label\"]\n",
        "data = df.toDF(*columns)\n"
      ],
      "metadata": {
        "id": "RmSDp0v1SAPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show(10)"
      ],
      "metadata": {
        "id": "4u9PZgHzSfn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "دستور ذیل یک دسته‌بندی روی داده‌ها بر اساس برچسب آنها انجام می‌دهد. این دسته‌بندی شهود خوبی نسبت به وضعیت داده‌ها ارائه می‌کند\n",
        "</p>"
      ],
      "metadata": {
        "id": "TlOlusLpX8ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "data.select('label').groupBy('label').count().orderBy(col('count').desc()).show(10)"
      ],
      "metadata": {
        "id": "UrPuguVsWbif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "برای انجام خوشه‌بندی لازم است تا داده‌های ثبت شده در DataFrame همگی به صورت عدد باشند. در حالیکه در بعضی ستون‌ها داده‌های داده شده به صورت string هستند. با اینکه می‌توان این داده‌ها را با کمک بعضی دستورات Spark به عدد تبدیل کرد، ولی برای سادگی در این پروژه ستون‌هایی که داده متنی دارند را حذف می‌کنیم:\n",
        "</p>"
      ],
      "metadata": {
        "id": "6EL7LAKJYI-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans, KMeansModel \n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n"
      ],
      "metadata": {
        "id": "us8o0yC3WpE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "حال داده‌های مورد نظر را لازم است تا به یک بردار عددی تبدیل کنیم. برای این منظور همه اطلاعات هر سطر (به جز ستون آخر) را در قالب یک ستون جدید برداری در نظر می‌گیریم.\n",
        "</p>"
      ],
      "metadata": {
        "id": "5KzijSYFYmIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).setOutputCol('featureVector')"
      ],
      "metadata": {
        "id": "Lh8TT9pVagPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "برای انجام خوشه‌بندی از الگوریتم KMeans استفاده می‌کنیم:\n",
        "</p>"
      ],
      "metadata": {
        "id": "0XadtwaWY9wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")"
      ],
      "metadata": {
        "id": "AMd1pV1Ya1g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "در ادامه با کمک ابزارهای موجود در پکیج یادگیری ماشین کار خوشه‌بندی را انجام می‌دهیم. \n",
        "این بخش کد را لازم است شما تکمیل کنید. برای این منظور پیشنهاد می‌شود که توضیحات اولیه درباره نحوه استفاده از توابع و مفاهیم مطرح در Spark برای استفاده از ابزارهای یادگیری ماشین را مطالعه کنید:\n",
        "<br>\n",
        "<a href=\"https://spark.apache.org/docs/latest/ml-guide.html\">https://spark.apache.org/docs/latest/ml-guide.html</a>\n",
        "<br>\n",
        "به طور مشخص پیشنهاد می‌شود که با مفاهیم Pipeline و Transform و Estimator آشنا شوید.\n",
        "<br>\n",
        "همچنین مطالعه بخش Clustering در راهنمای فوق می‌تواند مفید باشد.\n",
        "<br>\n",
        "</p>"
      ],
      "metadata": {
        "id": "7bcKObNIZMTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "حال، کد لازم برای انجام خوشه‌بندی داده‌های فوق به دو خوشه را بنویسید. \n",
        "<br>\n",
        "دقت کنید که برای این کار لازم است که از مفهوم Pipeline نیز استفاده کنید.\n",
        "</p>"
      ],
      "metadata": {
        "id": "E8p4nC3Bae0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Write your Code here."
      ],
      "metadata": {
        "id": "O_pCTOJfay-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "یکی از سوالات مهمی که در خوشه‌بندی KMeans می‌بایست پاسخ دهیم این است که عدد مناسب برای K چه مقداری می‌بایست باشد. برای پاسخ به این سوال، مشابه روشی که در کلاس توضیح داده شد، می‌توانیم به ازای K های مختلف خوشه‌بندی را انجام دهیم و  جایی که کاهش قابل توجهی در فاصله بین نقاطه و مراکز خوشه اتفاق افتاد را به عنوان مقدار K مطلوب در نظر بگیریم. برای توضیحات بیشتر به اسلایدهای درس مراجعه کنید.\n",
        "<br>\n",
        "می‌خواهیم کد لازم برای یافتن مقدار مطلوب K را بنویسیم. برای اینکار می‌توانیم از کتابخانه ClusterEvaluator در اسپارک استفاده کنیم. با مطالعه این کتابخانه و نوشتن کد Pipeline مناسب، الگوریتم KMeans را به ازای K های مختلف اجرا کرده و مشخص کنید که مقدار مطلوب برای K چه مقداری باید باشد. \n",
        "<br>\n",
        "با توجه به اینکه زمان اجرای خوشه‌بندی برای تعداد زیاد K می‌تواند بسیار طولانی باشد، پیشنهاد می‌شود که این آزمایش را برای مقادیر  40, 60, 80, 100, 120 انجام دهید.\n",
        "</p>"
      ],
      "metadata": {
        "id": "zqYLFyUYa4sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here."
      ],
      "metadata": {
        "id": "0tOH8TgMcMBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "مقدار مطلوب K چه عددی خواهد بود؟ چرا؟ پاسخ را در همینجا بنویسید.\n",
        "</p>"
      ],
      "metadata": {
        "id": "jEhcHZRcdY5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p dir=rtl>\n",
        "پاسخ سوال...\n",
        "\n",
        "</p>"
      ],
      "metadata": {
        "id": "SBtmZfDYdoGD"
      }
    }
  ]
}